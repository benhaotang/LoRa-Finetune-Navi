{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec02e398-3fd8-40a5-a7fd-8234ee4c1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AMD_SERIALIZE_KERNEL\"] = \"3\" \n",
    "os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = \"11.0.0\"\n",
    "os.environ[\"HIP_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a4eb59-83b3-452a-b340-447464292acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587cf442-7397-480e-b396-9ba0917094cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_llama_prompt(example):\n",
    "    \"\"\"Format data according to Llama's template\"\"\"\n",
    "    return f\"[INST] {example['instruction']} [/INST]\\n{example['output']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e956cd-fc6c-426d-8017-27d702f376f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer():\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model with AMD-specific settings\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        use_cache=False,\n",
    "        torch_dtype=torch.float32,  # Use full precision for AMD\n",
    "        # Add these for AMD compatibility\n",
    "        use_flash_attention_2=False,  # Disable flash attention\n",
    "        rope_scaling=None,  # Disable RoPE scaling\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\"\n",
    "        ],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b1a8d7d-1d1d-4967-8a35-cfbdd5b0f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training_args():\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"./llama_finetuned_{current_time}\"\n",
    "    \n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=2,  # Reduced for 1B model\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,  # For AMD GPU\n",
    "        logging_steps=10,\n",
    "        save_steps=50,\n",
    "        save_total_limit=3,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        load_best_model_at_end=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        warmup_steps=30,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=30,\n",
    "        per_device_eval_batch_size=2,\n",
    "        remove_unused_columns=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26df048e-e592-436c-8723-10a9379afa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset():\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"kejian/arxiv-physics-debug-v0\")\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        # Format prompts first - create a list of strings\n",
    "        formatted_texts = [format_llama_prompt({\n",
    "            \"instruction\": instruction,\n",
    "            \"output\": output\n",
    "        }) for instruction, output in zip(examples[\"instruction\"], examples[\"output\"])]\n",
    "        \n",
    "        # Tokenize the formatted texts\n",
    "        tokenized = tokenizer(\n",
    "            formatted_texts,\n",
    "            truncation=True,\n",
    "            max_length=2048,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    # Split dataset first\n",
    "    split_dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "    \n",
    "    print(\"Example of formatted text before tokenization:\")\n",
    "    print(format_llama_prompt(split_dataset[\"train\"][0]))\n",
    "    \n",
    "    # Tokenize both splits\n",
    "    train_tokenized = split_dataset[\"train\"].map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=split_dataset[\"train\"].column_names,\n",
    "        desc=\"Tokenizing train split\"\n",
    "    )\n",
    "    \n",
    "    eval_tokenized = split_dataset[\"test\"].map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=split_dataset[\"test\"].column_names,\n",
    "        desc=\"Tokenizing validation split\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining set size:\", len(train_tokenized))\n",
    "    print(\"Validation set size:\", len(eval_tokenized))\n",
    "    \n",
    "    return train_tokenized, eval_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1557e872-0769-411b-ad2d-7c300ee2e9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb1de2c27004ed0b17ef9be7a4b8843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c96d200d8044eb8505a5e3078ca985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eeb657c86f3497ca1b57e738f6cee50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33e268c41144a23a1fa35f6a910e1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/927 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af00f46919fd4f2db378d735cd713ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b135cfa30b949348c9030f82eea8057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "global tokenizer  # Add this line\n",
    "model, tokenizer = setup_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "100666a1-1696-4d48-ba43-c00e12f7b354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of formatted text before tokenization:\n",
      "[INST] Electron phonon transport in one dimensional junctions? [/INST]\n",
      "Electron-phonon transport in one-dimensional (1D) junctions refers to the transfer of energy and momentum between electrons and phonons in a narrow region of a 1D structure. This is an important phenomenon in nanoelectronics and semiconductor research, where the control and manipulation of electron and phonon transport is critical for developing high-performance devices.\n",
      "\n",
      "In 1D junctions, phonons (lattice vibrations) can scatter electrons, resulting in energy loss and a reduction in the electron's momentum. This process can result in a decrease in electrical conductivity and an increase in thermal conductivity of the junction.\n",
      "\n",
      "The strength of electron-phonon interactions in 1D junctions depends on various factors, such as the material properties, temperature, and the length scale of the junction. In general, electron-phonon scattering is more significant in materials with stronger electron-phonon coupling, higher phonon densities, and lower temperatures.\n",
      "\n",
      "To model electron-phonon transport in 1D junctions, one can use theoretical frameworks such as the Boltzmann transport equation or the Landauer formalism. These methods allow researchers to calculate the electron and phonon transport properties of the junction, including the conductance, thermal conductivity, and figure of merit (ZT), which is related to the efficiency of thermoelectric devices.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750913551e1f4bd292f1dedcc41d1c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train split:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd83671f2a341598c1ef2d40a8884cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation split:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set size: 450\n",
      "Validation set size: 50\n"
     ]
    }
   ],
   "source": [
    "train_dataset, eval_dataset = prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f44be21-ffcc-412d-8ffc-f8d782a33ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\"\n",
    "            ],  # Llama attention modules\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85670a7f-9125-4de0-9e3f-101ff5c287c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benhaotang/miniconda3/lib/python3.12/site-packages/transformers-4.45.0.dev0-py3.12.egg/transformers/training_args.py:1547: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = setup_training_args()\n",
    "trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,  # Add this line\n",
    "            data_collator=DataCollatorForLanguageModeling(\n",
    "                tokenizer=tokenizer,\n",
    "                mlm=False\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d24322b-ab62-4f93-87e7-56084959bac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 38:30, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.266700</td>\n",
       "      <td>1.323192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=84, training_loss=1.2797694546835763, metrics={'train_runtime': 2338.0306, 'train_samples_per_second': 0.577, 'train_steps_per_second': 0.036, 'total_flos': 1.6099718731923456e+16, 'train_loss': 1.2797694546835763, 'epoch': 2.986666666666667})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "141d0aec-2d8b-4944-ad8a-59372894f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/mnt/largefs/models/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23136c0c-6f63-4181-9570-0edd1910d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_path = \"/mnt/largefs/models/final_model\"\n",
    "trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)\n",
    "trainer.model.config.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bbe0fbf-fbef-450b-b43a-cbc1fee6e3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/benhaotang/llama3.2-1B-physics-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='benhaotang/llama3.2-1B-physics-finetuned')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "create_repo(\"benhaotang/llama3.2-1B-physics-finetuned\", private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13ee0b20-9406-4c32-9266-f53aca66df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"/mnt/largefs/models/final_model\"  \n",
    "base_model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "new_model_name = \"benhaotang/llama3.2-1B-physics-finetuned\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6368480c-bda5-4b7a-8af6-0ea7a2ecac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adapter model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading adapter model...\")\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "199a071d-e894-4cac-8a7b-648ea76dd650",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "836918de-f26c-4a86-84fc-b0706ed51794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging model with adapter...\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging model with adapter...\")\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea5c64d1-2691-4008-bbdf-76250aa001a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model card\n",
    "model_card = f\"\"\"\n",
    "# Llama 3.2 Physics Fine-tuned Model\n",
    "\n",
    "This model is a fine-tuned version of {base_model_name} on kejian/arxiv-physics-debug-v0.\n",
    "\n",
    "## Model description\n",
    "- Base model: {base_model_name}\n",
    "- Training data: kejian/arxiv-physics-debug-v0\n",
    "- Fine-tuning type: LoRA\n",
    "- Use case: Physics domain questions\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{new_model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{new_model_name}\")\n",
    "\n",
    "# Example usage\n",
    "text = \"[INST] What is rg flow? [/INST]\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "# Save model card\n",
    "with open(\"README.md\", \"w\") as f:\n",
    "    f.write(model_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a41bf22e-f8e7-4136-ad9d-32a64467cf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing to Hugging Face Hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benhaotang/miniconda3/lib/python3.12/site-packages/transformers-4.45.0.dev0-py3.12.egg/transformers/utils/hub.py:894: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1591687c7e48c4acbb3e8246a68174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/998M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2724d318c4534ac293f8c2f0a225d274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6212faf80df24b0992459f769be42eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6543942096b24d5d8e6f80ecdb714891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/973M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab94d053ff5f49528a38b01b867b4c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33948f725b39411fbcfc5b8dd6bda22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/931M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benhaotang/miniconda3/lib/python3.12/site-packages/transformers-4.45.0.dev0-py3.12.egg/transformers/utils/hub.py:894: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f4a2aff02f4947a84a16e40ace257d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully pushed to: https://huggingface.co/benhaotang/llama3.2-1B-physics-finetuned\n"
     ]
    }
   ],
   "source": [
    "print(\"Pushing to Hugging Face Hub...\")\n",
    "merged_model.push_to_hub(\n",
    "    new_model_name,\n",
    "    use_auth_token=True,\n",
    "    max_shard_size=\"1GB\",  # Split into smaller files\n",
    "    safe_serialization=True\n",
    ")\n",
    "tokenizer.push_to_hub(new_model_name, use_auth_token=True)\n",
    "\n",
    "print(f\"Model successfully pushed to: https://huggingface.co/{new_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58680bc4-adc4-4902-820f-73e959f046aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd3ea642-8bcf-4efc-8f99-501660bfd2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fb60d4f61149a9a9eb44dcae947758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7655ef5dc8b3402f89abee47c11667ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me a short intodcution to renormalization group(RG) flow in physcis?\n",
      "I'll start by explaining the concept of renormalization group flow, then we can discuss the different types of renormalization group flow and their applications.\n",
      "\n",
      "## Step 1: Introduction to Renormalization Group (RG) Flow\n",
      "The Renormalization Group (RG) is a theoretical framework used in physics to study the behavior of physical systems at very small distances or high energies. It is a powerful tool for understanding the dynamics of systems that exhibit scale invariance, meaning that their properties remain the same under certain transformations of their parameters.\n",
      "\n",
      "## Step 2: RG Flow in Physics\n",
      "In physics, the RG flow is a mathematical transformation that describes how physical parameters, such as energy or length, evolve over time or space in a system. It is typically represented as a map between a set of initial parameters and a set of final parameters, where the map is defined by a set of equations that describe how the parameters change as the system evolves. The RG flow is often used to study the behavior of systems at the critical point, where the system's properties change dramatically as the parameters change.\n",
      "\n",
      "## Step 3: Types of RG Flow\n",
      "There are several types of RG flow, including:\n",
      "\n",
      "* **Continuum RG Flow**: This is the most common type of RG flow, where the system is treated as a continuum, and the equations of motion are derived from a set of partial differential equations.\n",
      "* **Discrete RG Flow**: This type of RG flow is used in systems with discrete variables, such as lattice gauge theory.\n",
      "* **Asymptotic RG Flow**: This type of RG flow is used in systems with a power-law dependence of the parameters on the energy scale.\n",
      "* **Non-Asymptotic RG Flow**: This type of RG flow is used in systems with a logarithmic dependence of the parameters on the energy scale.\n",
      "\n",
      "## Step 4: Applications of RG Flow\n",
      "The RG flow is a powerful tool for understanding the behavior of physical systems at very small distances or high energies. Some of the applications of RG flow include:\n",
      "\n",
      "* **Quantum Field Theory**: RG flow is used to study the behavior of quantum field theories, such as the Standard Model of particle physics.\n",
      "* **Condensed Matter Physics**: RG flow is used to study the behavior of materials at the nanoscale, such as superconductors and superfluids.\n",
      "* **High-Energy Physics**: RG flow is used to study the behavior of high-energy particles, such as quarks and leptons.\n",
      "\n",
      "The final answer is: There is no final answer, as this is an introductory discussion on the Renormalization Group (RG) flow in physics.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"benhaotang/llama3.2-1B-physics-finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"benhaotang/llama3.2-1B-physics-finetuned\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Example usage\n",
    "text = \"Give me a short intodcution to renormalization group(RG) flow in physcis?\\n\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=2048)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d360f6-4493-4d24-8193-1a2de07e12fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py312)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
